{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install BitsAndBytes liger_kernel deepspeed lightning gdown peft mpi4py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:49:24.617428Z","iopub.execute_input":"2025-04-27T10:49:24.617647Z","iopub.status.idle":"2025-04-27T10:53:53.126946Z","shell.execute_reply.started":"2025-04-27T10:49:24.617631Z","shell.execute_reply":"2025-04-27T10:53:53.126010Z"}},"outputs":[{"name":"stdout","text":"Collecting BitsAndBytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nCollecting liger_kernel\n  Downloading liger_kernel-0.5.8-py3-none-any.whl.metadata (23 kB)\nCollecting deepspeed\n  Downloading deepspeed-0.16.7.tar.gz (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting lightning\n  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting mpi4py\n  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from BitsAndBytes) (2.5.1+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from BitsAndBytes) (1.26.4)\nRequirement already satisfied: triton>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from liger_kernel) (3.1.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\nCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.570.86)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.2)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.3)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.13.1)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.16)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->BitsAndBytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->BitsAndBytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->BitsAndBytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->BitsAndBytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->BitsAndBytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->BitsAndBytes) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->BitsAndBytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->BitsAndBytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->BitsAndBytes) (1.3.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->BitsAndBytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->BitsAndBytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->BitsAndBytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->BitsAndBytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->BitsAndBytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->BitsAndBytes) (2024.2.0)\nDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading liger_kernel-0.5.8-py3-none-any.whl (150 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed, mpi4py\n  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for deepspeed: filename=deepspeed-0.16.7-py3-none-any.whl size=1642799 sha256=94a50d801f4af63dc71e7a3a3007f0f5db1590b27b8fa72f8d9ef9d3ef049983\n  Stored in directory: /root/.cache/pip/wheels/42/e7/1a/2106f7197cc13e09c68f1b4f55f7e5117a985e726378968970\n  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4458268 sha256=e616cf9bd7cbc86444bc7967116ea13c019f10617110c6291035960659d98011\n  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\nSuccessfully built deepspeed mpi4py\nInstalling collected packages: hjson, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, mpi4py, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, liger_kernel, lightning, deepspeed, BitsAndBytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed BitsAndBytes-0.45.5 deepspeed-0.16.7 hjson-3.1.0 liger_kernel-0.5.8 lightning-2.5.1.post0 mpi4py-4.0.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile dp.json\n{\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_param\": {\n            \"device\": \"none\"\n        },\n        \"offload_optimizer\": {\n            \"device\": \"none\"\n        },\n        \"stage3_param_persistence_threshold\": 1e4,\n        \"stage3_max_live_parameters\": 3e7,\n        \"stage3_prefetch_bucket_size\": 3e7,\n        \"memory_efficient_linear\": false\n    },\n    \"bf16\": {\n        \"enabled\": true,\n        \"loss_scale_window\": 50,\n        \"min_loss_scale\": 1e-10\n    },\n    \"gradient_clipping\": 1.0,\n    \"gradient_accumulation_steps\": \"auto\",\n    \"prescale_gradients\": false,\n    \"wall_clock_breakdown\": false,\n    \"hybrid_engine\": {\n        \"enabled\": false,\n        \"max_out_tokens\": 512,\n        \"inference_tp_size\": 1,\n        \"release_inference_cache\": false,\n        \"pin_parameters\": true,\n        \"tp_gather_partition_size\": 8\n    }\n    \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.128519Z","iopub.execute_input":"2025-04-27T10:53:53.128796Z","iopub.status.idle":"2025-04-27T10:53:53.135146Z","shell.execute_reply.started":"2025-04-27T10:53:53.128772Z","shell.execute_reply":"2025-04-27T10:53:53.134411Z"}},"outputs":[{"name":"stdout","text":"Writing dp.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile model.py\nfrom transformers import AutoModelForCausalLM, AutoModel, AutoConfig\nimport torch\nfrom transformers import AutoTokenizer\nfrom torch import nn\nfrom transformers.models.mt5.modeling_mt5 import MT5Attention\nimport torch.nn.functional as F\nfrom peft import get_peft_model, LoraConfig, PeftModel, PeftConfig\n\n\nclass MLP(nn.Module):\n    def __init__(self, mt_dim, llm_dim):\n        super(MLP, self).__init__()\n        self.linear1 = nn.Linear(mt_dim, mt_dim * 2)\n        self.linear2 = nn.Linear(mt_dim * 2, llm_dim)\n        self.relu = nn.ReLU()\n    def forward(self, mt_hidden_state):\n        output = self.linear1(mt_hidden_state)\n        output = self.relu(output)\n        output = self.linear2(output)\n        return output\n\nclass Mapping(nn.Module):\n    def __init__(self, mt_dim, llm_dim):\n        super(Mapping, self).__init__()\n        self.mlp = MLP(mt_dim, llm_dim)\n        self.end_boundary = nn.Parameter(\n            torch.zeros(1, 1, llm_dim), requires_grad=True\n        )\n    def forward(self, hidden_states):\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states\n\n    def get_embed(self):\n        return self.end_boundary\n\nclass BiasTunedLinear(nn.Module):\n    \"\"\"Wrap nn.Linear, re-expose its weight/bias, and add a tiny bias_delta + scale_tuned.\"\"\"\n    def __init__(self, linear: nn.Linear):\n        super().__init__()\n        # keep the pretrained linear (with its .weight and .bias)\n        self.linear = linear\n        # re-expose so HF code still sees `.weight` and `.bias`\n        self.weight = linear.weight\n        self.bias   = linear.bias\n\n        # our new, small tuning parameters\n        self.bias_delta = nn.Parameter(torch.zeros(self.linear.out_features))\n        self.scale_tuned = nn.Parameter(torch.ones(1))\n\n    def forward(self, x):\n        orig = self.linear(x)                   # -> W x + b\n        return self.scale_tuned * (orig + self.bias_delta)\n\ndef apply_bias_tuning(module: nn.Module):\n    \"\"\"Recursively replace every nn.Linear with a BiasTunedLinear.\"\"\"\n    for name, child in list(module.named_children()):\n        if isinstance(child, nn.Linear):\n            setattr(module, name, BiasTunedLinear(child))\n        else:\n            apply_bias_tuning(child)\n\nclass GatedMT5Attention(MT5Attention):\n    def __init__(self, config, prompt_length, hidden_size, pretrained_attention=None, has_relative_attention_bias=False):\n        super().__init__(config, has_relative_attention_bias)\n        if pretrained_attention is not None:\n            self.load_state_dict(pretrained_attention.state_dict(), strict=False)\n        self.gate = nn.Parameter(torch.zeros(self.n_heads))\n        self.prompts = nn.Parameter(torch.zeros(prompt_length, hidden_size))\n\n    def forward(\n        self,\n        hidden_states,\n        mask=None,\n        key_value_states=None,\n        position_bias=None,\n        past_key_value=None,\n        layer_head_mask=None,\n        query_length=None,\n        use_cache=False,\n        output_attentions=False,\n        cache_position=None,\n    ):\n        \"\"\"\n        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n        \"\"\"\n        # Input is (batch_size, seq_length, dim)\n        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n        is_cross_attention = key_value_states is not None\n        \n        query_states = self.q(hidden_states)\n        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n\n        prompt_embeds = self.prompts\n        prompt_length = prompt_embeds.size(0)\n        prompt = prompt_embeds.unsqueeze(0).expand(batch_size, -1, -1)\n        hidden_states = torch.cat([prompt, hidden_states], dim=1)\n\n\n        current_states = key_value_states if is_cross_attention else hidden_states\n        key_states = self.k(current_states)\n        value_states = self.v(current_states)\n        key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n        value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n\n        prompt_key = key_states[:, :, :prompt_length, :]\n        token_key = key_states[:, :, prompt_length:, :]\n        prompt_value = value_states[:, :, :prompt_length, :]\n        token_value = value_states[:, :, prompt_length:, :]\n\n        prompt_scores = torch.matmul(query_states, prompt_key.transpose(3, 2))\n        scores = torch.matmul(query_states, token_key.transpose(3, 2))\n        \n        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n        # scores = torch.matmul(query_states, key_states.transpose(3, 2))\n\n        if position_bias is None:\n            key_length = token_key.shape[-2]\n            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n            if not self.has_relative_attention_bias:\n                position_bias = torch.zeros(\n                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                )\n                if self.gradient_checkpointing and self.training:\n                    position_bias.requires_grad = True\n            else:\n                position_bias = self.compute_bias(\n                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n                )\n                position_bias = position_bias[:, :, -seq_length:, :]\n\n            if mask is not None:\n                causal_mask = mask[:, :, :, : token_key.shape[-2]]\n                position_bias = position_bias + causal_mask\n\n        if self.pruned_heads:\n            mask = torch.ones(position_bias.shape[1])\n            mask[list(self.pruned_heads)] = 0\n            position_bias_masked = position_bias[:, mask.bool()]\n        else:\n            position_bias_masked = position_bias\n\n        scores += position_bias_masked\n\n        attn_weights_prompts = nn.functional.softmax(prompt_scores.float(), dim=-1).type_as(prompt_scores)\n        gated_prompt_scores = torch.tanh(self.gate).unsqueeze(0).unsqueeze(2).unsqueeze(3) * attn_weights_prompts\n        \n        # (batch_size, n_heads, seq_length, key_length)\n        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        attn_weights = torch.cat([gated_prompt_scores, attn_weights], dim=-1)\n        value_states = torch.cat([prompt_value, token_value], dim=2)\n        \n        attn_output = torch.matmul(attn_weights, value_states)\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n        attn_output = self.o(attn_output)\n\n        outputs = (attn_output, past_key_value, position_bias)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n        return outputs\n\nclass MultilingualEmbeddingModel(nn.Module):\n    ALLOWED_MODELS = {\n        \"google/mt5-small\",\n        \"google/mt5-base\",\n        \"google/mt5-large\",\n        \"google/mt5-xl\",\n        \"DKYoon/mt5-small-lm-adapt\",\n        \"DKYoon/mt5-large-lm-adapt\",\n        \"DKYoon/mt5-xl-lm-adapt\",\n    }\n    \n    def __init__(self, embedding_model, max_seq_len):\n        super().__init__()\n\n        if embedding_model not in self.ALLOWED_MODELS:\n            raise ValueError(f\"Model is not in allowed models: {self.ALLOWED_MODELS}\")\n        \n        self.embedding_model = AutoModel.from_pretrained(embedding_model)\n        self.embedding_model = self.embedding_model.encoder \n            \n        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n        \n        self.embedding_dim = self.embedding_model.config.hidden_size\n\n        self.max_seq_len = max_seq_len\n\n        self.num_layers = len(self.embedding_model.block)\n\n        self.prompt_length = 10\n\n        prompt_layers = [-4,-3,-2,-1]\n        \n        for idx in prompt_layers:\n            attention_weights = self.embedding_model.block[idx].layer[0].SelfAttention\n            self.embedding_model.block[idx].layer[0].SelfAttention = GatedMT5Attention(self.embedding_model.config, self.prompt_length, self.embedding_dim, pretrained_attention=attention_weights)\n\n        apply_bias_tuning(self.embedding_model)\n        \n        for n, p in self.named_parameters():\n            if any(tok in n for tok in (\n                \".bias\",        # bias_tuned_linear.bias\n                \".scale\",       # bias_tuned_linear.scale\n                \"prompts\",      # gated prompts\n                \"gate\",         # gated heads\n                \"layer_norm\",   # MT5 layer norms\n            )):\n                p.requires_grad = True\n            else:\n                p.requires_grad = False\n    \n    def get_input_embeddings(self, model, input_ids):\n        return model.get_input_embeddings()(input_ids)\n    \n    def get_last_hidden_states(self, encoded_inputs, model, tokenizer):\n        input_ids, attention_mask = self.mt_input_features(encoded_inputs, tokenizer)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.last_hidden_state, attention_mask\n\n    def mt_input_features(self, input_texts_m2m, tokenizer):\n        input_ids_m2m, attention_mask_m2m = [], []\n        for input_text_m2m in input_texts_m2m:\n            encoding_m2m = tokenizer(input_text_m2m,\n                                         padding='longest',\n                                         max_length=self.max_seq_len,\n                                         truncation=True)\n            input_ids_m2m_item = encoding_m2m.input_ids\n            attention_mask_m2m_item = encoding_m2m.attention_mask\n            input_ids_m2m.append(input_ids_m2m_item)\n            attention_mask_m2m.append(attention_mask_m2m_item)\n        max_len = max([len(item) for item in input_ids_m2m])\n        m2m_pad_id = tokenizer.pad_token_id\n        for input_ids_m2m_item, attention_mask_m2m_item in zip(input_ids_m2m, attention_mask_m2m):\n            while len(input_ids_m2m_item) < max_len:\n                input_ids_m2m_item.append(m2m_pad_id)\n                attention_mask_m2m_item.append(0)\n        input_ids_m2m = torch.tensor(input_ids_m2m, dtype=torch.long).cuda()\n        attention_mask_m2m = torch.tensor(attention_mask_m2m, dtype=torch.long).cuda()\n        return input_ids_m2m, attention_mask_m2m\n    \n    def forward(self, encoded_inputs):\n        embeddings, attention_mask = self.get_last_hidden_states(\n            encoded_inputs, \n            self.embedding_model, \n            self.tokenizer,\n        )\n        \n        return embeddings, attention_mask\n\nclass MPTModel(nn.Module):\n    def __init__(self, config):\n        super(MPTModel, self).__init__()\n        self.config = config  # Ensure there is a config attribute\n        self.max_gen_len = config['max_gen_len']\n        self.encoder_mt = MultilingualEmbeddingModel(config['mt_path'], config['max_seq_len'])\n        \n        model_llm = AutoModelForCausalLM.from_pretrained(config['llm_path'])\n\n        self.model_llm = model_llm\n\n        self.lora_config = LoraConfig(\n            r=32,\n            lora_alpha=128,  # Maintains an effective scaling factor of 4\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n            lora_dropout=0.1,\n            bias=\"all\",\n            task_type=\"CAUSAL_LM\",\n        )\n\n        self.llm_embedding_layer = self.model_llm.get_input_embeddings()\n        for name, parameter in self.model_llm.named_parameters():\n            parameter.requires_grad = False\n\n        # self.model_llm = get_peft_model(self.model_llm, self.lora_config)\n\n        # for name, param in self.model_llm.named_parameters():\n        #     if \"lora\" in name:\n        #         param.requires_grad = True  \n\n        d_model = self.encoder_mt.embedding_dim\n        self.mapping = Mapping(d_model, model_llm.config.hidden_size)\n        self.llm_pad_token_id = config['llm_pad_token_id']\n        self.llm_bos_token_id = config['llm_bos_token_id']\n        print('mapping layer size:', sum(param.numel() for param in self.mapping.parameters()) / 1000000)\n\n    def squeeze_pad(self, hidden_states, masks):\n        x_01 = (masks != 0).long()\n\n        seq_num_len = x_01.size(1)\n        offset = torch.tensor([(i + 1) for i in range(seq_num_len)], dtype=torch.long).to(x_01.device)\n        offset = offset.unsqueeze(dim=0).expand_as(x_01)\n        x_01 *= offset\n        _, idx = x_01.sort(1, descending=False)\n\n        masks = masks.gather(1, idx)\n        idx = idx.unsqueeze(dim=-1).expand_as(hidden_states)\n        hidden_states = hidden_states.gather(1, idx)\n\n        bs, seq_len, dim = hidden_states.size()\n        masks_sum = torch.sum(masks, dim=0)\n        idx = masks_sum > 0\n        idx = idx.unsqueeze(dim=0).expand_as(masks)\n        masks = masks[idx]\n        idx_ex = idx.unsqueeze(dim=-1).expand_as(hidden_states)\n        hidden_states = hidden_states[idx_ex]\n        hidden_states = hidden_states.view(bs, -1, dim)\n        masks = masks.view(bs, -1)\n\n        return hidden_states, masks, idx\n\n    def merge_loara(self):\n        self.model_llm = self.model_llm.merge_and_unload()\n        self.model_llm.save_pretrained('/kaggle/working/llama_1b_xcsqa/')\n\n    def forward(self, encoded_inputs,\n                labels=None, mask_label=None, input_ids_prompt=None, mask_prompt=None):\n        end_boundary = self.mapping.get_embed()\n        bs = len(encoded_inputs)\n        end_boundary = end_boundary.expand([bs, 1, end_boundary.size(-1)])\n\n        if self.llm_bos_token_id is None:\n            bos = torch.tensor([self.llm_pad_token_id for i in range(bs)], dtype=torch.long).cuda()\n            mask = torch.zeros([bs, 1], dtype=torch.long).cuda()\n        else:\n            bos = torch.tensor([self.llm_bos_token_id for i in range(bs)], dtype=torch.long).cuda()\n            mask = torch.ones([bs, 1], dtype=torch.long).cuda()\n        bos_embedding = self.llm_embedding_layer(bos)\n        bos_embedding = bos_embedding.view(bs, 1, -1)\n        llm_input_embedding = bos_embedding\n        llm_input_mask = mask\n\n        mt_encoder_outputs, attention_mask_mt = self.encoder_mt(encoded_inputs)\n        \n        mt_hidden_state = self.mapping(mt_encoder_outputs)\n        llm_input_embedding = torch.cat([llm_input_embedding, mt_hidden_state, end_boundary],\n                                        dim=1)\n        llm_input_mask = torch.cat([llm_input_mask, attention_mask_mt, mask], dim=1)\n\n        if input_ids_prompt is not None:\n\n            hidden_states_prompt = self.llm_embedding_layer(input_ids_prompt)\n            llm_input_embedding = torch.cat([llm_input_embedding, hidden_states_prompt], dim=1)\n            llm_input_mask = torch.cat([llm_input_mask, mask_prompt], dim=1)\n        if labels is not None:\n            pad_labels = llm_input_mask * -100 + (1 - llm_input_mask) * -100\n            label_embedding = self.llm_embedding_layer(labels)\n            llm_input_embedding = torch.cat([llm_input_embedding, label_embedding], dim=1)\n            llm_input_mask = torch.cat([llm_input_mask, mask_label], dim=1)\n            labels = labels * mask_label - 100 * (1 - mask_label)\n            labels = torch.cat([pad_labels, labels], dim=1)\n\n        llm_input_embedding, llm_input_mask, cut_pad_idx \\\n            = self.squeeze_pad(llm_input_embedding, llm_input_mask)\n\n        if labels is None:\n            generate_ids = self.model_llm.generate(inputs_embeds=llm_input_embedding,\n                                                   attention_mask=llm_input_mask,\n                                                   max_new_tokens=self.max_gen_len,\n                                                   pad_token_id=self.llm_pad_token_id,\n                                                   do_sample=False)\n            return generate_ids\n        else:\n            bs, seq_len = labels.size()\n            labels = labels[cut_pad_idx]\n            labels = labels.view(bs, -1)\n            output = self.model_llm(inputs_embeds=llm_input_embedding,\n                                    attention_mask=llm_input_mask,\n                                    labels=labels)\n            return output.loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.135959Z","iopub.execute_input":"2025-04-27T10:53:53.136195Z","iopub.status.idle":"2025-04-27T10:53:53.166987Z","shell.execute_reply.started":"2025-04-27T10:53:53.136176Z","shell.execute_reply":"2025-04-27T10:53:53.166286Z"}},"outputs":[{"name":"stdout","text":"Writing model.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile read_data.py\nimport math\nimport os\nimport random\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader, SequentialSampler\nimport json\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login, upload_file\n\nclass ExperimentDataset(Dataset):\n    def __init__(self, dataset) -> None:\n        super().__init__()\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        sample = self.dataset[idx]\n        return sample\n\ndef llm_input_features(input_texts_llm, tokenizer_llm,\n                         max_seq_len, add_bos_token, add_eos_token):\n    tokenizer_llm.add_bos_token = add_bos_token\n    # tokenizer_llm.add_eos_token = add_eos_token\n    if add_eos_token:\n        input_texts_llm = [f\"{prompt}{tokenizer_llm.eos_token}\" for prompt in input_texts_llm]\n    encoding_llm = tokenizer_llm(input_texts_llm,\n                         padding='longest',\n                         max_length=max_seq_len,\n                         truncation=True,\n                         add_special_tokens = False,\n                         return_tensors=\"pt\")\n    input_ids_llm = encoding_llm.input_ids.cuda()\n    attention_mask_llm = encoding_llm.attention_mask.cuda()\n    attention_mask_llm[:,-1] = 1.0\n    return input_ids_llm, attention_mask_llm\n\ndef read_dataset(path):\n    if 'jsonl' in path:\n        dataset = []\n        with open(path, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n            for line in lines:\n                dataset.append(json.loads(line))\n    elif 'json' in path:\n        with open(path, 'r', encoding='utf-8') as f:\n            dataset = json.load(f)\n        if isinstance(dataset, dict):\n            if 'data' in dataset:\n                dataset = dataset['data']\n    else:\n        with open(path, 'r', encoding='utf-8') as f:\n            dataset = f.readlines()\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.168499Z","iopub.execute_input":"2025-04-27T10:53:53.168746Z","iopub.status.idle":"2025-04-27T10:53:53.184834Z","shell.execute_reply.started":"2025-04-27T10:53:53.168729Z","shell.execute_reply":"2025-04-27T10:53:53.184045Z"}},"outputs":[{"name":"stdout","text":"Writing read_data.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile utils.py\nimport os\nimport random\nimport torch\nfrom tqdm import tqdm\nfrom read_data import llm_input_features\nimport numpy as np\n\ndef set_seed(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)  # cpu\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # gpu\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True  # consistent results on the cpu and gpu\n\ndef save_with_accelerate(accelerator, model, output_dir, model_name='pytorch_model.bin'):\n    os.makedirs(output_dir, exist_ok=True)\n    output_file = output_dir\n    accelerator.wait_for_everyone()\n    accelerator.save_model(model, output_file, max_shard_size=\"30GB\",safe_serialization=False)\n\n\ndef get_train_ds_config(train_batch_size=1,\n                        train_micro_batch_size_per_gpu=1,\n                        lr=2e-5,\n                        gradient_accumulation_steps=1,\n                        offload=True,\n                        stage=2,\n                        enable_hybrid_engine=False,\n                        inference_tp_size=1,\n                        release_inference_cache=False,\n                        pin_parameters=True,\n                        tp_gather_partition_size=8,\n                        max_out_tokens=512,\n                        warm_step=0,\n                        train_step=0):\n\n    device = \"cpu\" if offload else \"none\"\n    zero_opt_dict = {\n        \"stage\": stage,\n        \"offload_param\": {\n            \"device\": device\n        },\n        \"offload_optimizer\": {\n            \"device\": device\n        },\n        \"stage3_param_persistence_threshold\": 1e4,\n        \"stage3_max_live_parameters\": 3e7,\n        \"stage3_prefetch_bucket_size\": 3e7,\n        \"memory_efficient_linear\": False\n    }\n    return {\n        \"train_batch_size\": train_batch_size,\n        \"train_micro_batch_size_per_gpu\": train_micro_batch_size_per_gpu,\n        \"steps_per_print\": 2000,\n        \"zero_optimization\": zero_opt_dict,\n        \"bf16\": {\n            \"enabled\": False,\n        },\n        \"gradient_clipping\": 1.0,\n        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n        \"prescale_gradients\": False,\n        \"wall_clock_breakdown\": False,\n        \"hybrid_engine\": {\n            \"enabled\": enable_hybrid_engine,\n            \"max_out_tokens\": max_out_tokens,\n            \"inference_tp_size\": inference_tp_size,\n            \"release_inference_cache\": release_inference_cache,\n            \"pin_parameters\": pin_parameters,\n            \"tp_gather_partition_size\": tp_gather_partition_size,\n        },\n        \"optimizer\": {\n            \"type\": \"AdamW\",\n            \"params\": {\n                \"lr\": lr,\n                \"betas\": [\n                    0.8,\n                    0.999\n                ],\n                \"eps\": 1e-8\n            }\n        },\n        \"scheduler\": {\n            \"type\": \"WarmupCosineLR\",\n            \"params\": {\n            \"total_num_steps\": train_step,\n            \"warmup_num_steps\": warm_step\n            }\n        },\n    }\n\ndef evaluate_classification(model, test_set, tokenizer_llm, max_gen_len, use_prompt):\n    model.eval()\n    results_list = []\n    hit = 0\n    step_trange = tqdm(test_set)\n    preds, golds = [], []\n    for test_step in step_trange:\n        prompts = test_step['prompt']\n        targets = test_step['target']\n        input_ids_prompt, mask_prompt = None, None\n        if use_prompt:\n            add_bos_token = False\n            add_eos_token = False\n            input_ids_prompt, mask_prompt = llm_input_features(prompts, tokenizer_llm, max_gen_len, add_bos_token,\n                                                           add_eos_token)\n        generate_ids = model(prompts,\n                             input_ids_prompt=input_ids_prompt,\n                             mask_prompt=mask_prompt)\n\n        results = tokenizer_llm.batch_decode(generate_ids,\n                                               skip_special_tokens=True,\n                                               clean_up_tokenization_spaces=False)\n\n        preds += results\n        golds += targets\n\n        for result, prompt, target in zip(results, prompts, targets):\n            result = result.strip()\n            results_list.append({\n                'prompt': prompt,\n                'prediction': result,\n                'answer': target\n            })\n            if target == result:\n                hit += 1\n\n        acc = round(hit / len(results_list) * 100, 2)\n        loss_show = 'Acc:' + str(acc)\n        step_trange.set_postfix_str(loss_show)\n\n    acc = round(hit / len(results_list) * 100, 2)\n    return acc, results_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.185554Z","iopub.execute_input":"2025-04-27T10:53:53.185780Z","iopub.status.idle":"2025-04-27T10:53:53.204267Z","shell.execute_reply.started":"2025-04-27T10:53:53.185762Z","shell.execute_reply":"2025-04-27T10:53:53.203646Z"}},"outputs":[{"name":"stdout","text":"Writing utils.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile trainer.py\nimport os\nimport random\nfrom accelerate import Accelerator\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom transformers import AutoTokenizer, get_scheduler\nfrom datasets import load_dataset\nfrom huggingface_hub import login, upload_file\nimport json\nimport torch\nimport math\nfrom model import MPTModel\nfrom utils import set_seed, save_with_accelerate\nfrom read_data import ExperimentDataset, llm_input_features, read_dataset\n\ndef construct_prompt(sample):\n    return f\"### Instruction:\\nNews Sentence: {sample}\\nClassify the given news sentence into one of the following categories.\\nBusiness, Entertainment, Political, Sports, Science.\\n\\n### Response:\"\n\n\nclass Arguments:\n    def __init__(self):\n        BATCH_SIZE_PER_GPU=8\n        TOTAL_BATCH_SIZE=8\n        GRADIENT_ACC_STEPS = TOTAL_BATCH_SIZE // BATCH_SIZE_PER_GPU\n\n        self.llm_path = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n        self.mt_path = \"google/mt5-large\"\n        self.train_num = 8888\n        self.dev_size = 1000\n        self.lr = 3e-5\n        self.epoch_num = 3\n        self.gradient_accumulation = GRADIENT_ACC_STEPS\n        self.max_seq_len = 200\n        self.max_gen_len = 200\n        self.train_batch_size = TOTAL_BATCH_SIZE\n        self.eval_batch_size = BATCH_SIZE_PER_GPU\n        self.train_micro_batch_size_per_gpu = BATCH_SIZE_PER_GPU\n        self.augmentation = False\n        self.save_name = 'news'\n        self.stage_name = 'no_aug'\n        self.report_to = 'wandb'\n        self.logging_steps = 100\n        self.warm_rate = 0.05\n        self.lr_scheduler_name = 'cosine'\n        self.system_prompt = None\n        self.init_checkpoint = None\n\ndef main():\n    \n    args = Arguments()\n    \n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    accelerator_log_kwargs = {}\n    \n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation,\n        **accelerator_log_kwargs\n    )\n\n    accelerator.wait_for_everyone()\n    set_seed(0)\n\n    llm_path = args.llm_path\n    mt_path = args.mt_path\n    \n    token = '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'\n    login(token=token)\n\n    train_samples = []\n\n    ds = load_dataset(\"Themira/en_si_news_classification_with_label_name\")\n\n    train_ds = ds['train_en']\n        \n    for i in range(len(train_ds)):\n        example = train_ds[i]\n        sample = {}\n        sample['prompt'] = construct_prompt(example['sentence'])\n        sample['target'] = example['label']\n        train_samples.append(sample)\n\n    args.train_num = len(train_samples)\n    \n    train_set = train_samples[args.dev_size:]\n    dev_set = train_samples[:args.dev_size]\n    \n    dev_set = ExperimentDataset(\n        dev_set\n    )\n    \n    train_set = ExperimentDataset(\n        train_set\n    )\n\n    train_num = args.train_num\n    lr = args.lr\n    epoch_num = args.epoch_num\n    gradient_accumulation = args.gradient_accumulation\n    max_seq_len = args.max_seq_len\n    max_gen_len = args.max_gen_len\n\n    train_batch_size = args.train_batch_size\n    eval_batch_size = args.eval_batch_size\n    train_micro_batch_size_per_gpu = args.train_micro_batch_size_per_gpu\n\n    augmentation = args.augmentation\n    save_name = args.save_name\n    stage_name = args.stage_name\n    result_path_base = f'./results/{save_name}/{stage_name}/'\n    output_model_path_base = f'./outputs/{save_name}/{stage_name}/'\n    tokenizer_m2m = AutoTokenizer.from_pretrained(mt_path)\n    tokenizer_llm = AutoTokenizer.from_pretrained(llm_path, use_fast=True)\n    tokenizer_llm.pad_token = tokenizer_llm.eos_token\n    tokenizer_llm.padding_side = \"left\"\n    # tokenizer_llm.pad_token = \"[PAD]\"\n\n    print(json.dumps({\n        'llm_path': llm_path,\n        'mt_path': mt_path,\n        'lr': lr,\n        'epoch_num': epoch_num,\n        'gradient_accumulation': gradient_accumulation,\n        'train_set:': len(train_set),\n        'dev_set:': len(dev_set),\n        'max_seq_len': max_seq_len,\n        'max_gen_len': max_gen_len,\n        'train_batch_size': train_batch_size,\n        'result_path': result_path_base,\n        'output_model_path': output_model_path_base,\n    }, indent=2))\n\n\n    model_config = {\n        'mt_path': mt_path,\n        'llm_path': llm_path,\n        'max_gen_len': max_gen_len,\n        'llm_bos_token_id': tokenizer_llm.bos_token_id,\n        'llm_pad_token_id': tokenizer_llm.pad_token_id,\n        'augmentation' :  augmentation,\n        'max_seq_len': max_seq_len\n    }\n\n    model = MPTModel(model_config)\n\n    if args.init_checkpoint is not None:\n        init_checkpoint = args.init_checkpoint\n        checkpoint = torch.load(init_checkpoint, map_location='cpu')\n        #model_dict = checkpoint['model_state_dict']\n        model.load_state_dict(checkpoint, True)\n        print('mapping init from:', init_checkpoint)\n    print(model)\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(f\"Parameter name: {name}, requires_grad={param.requires_grad}, shape={param.shape}\")\n    #train_sampler = RandomSampler(train_set)\n    dev_sampler = SequentialSampler(dev_set)\n    train_dataloader = DataLoader(\n        dataset=train_set,\n        batch_size=train_micro_batch_size_per_gpu,\n        shuffle=True\n    )\n    \n    dev_dataloader = DataLoader(\n        dataset=dev_set,\n        batch_size=eval_batch_size,\n        shuffle=False,\n        sampler=dev_sampler,\n        num_workers=1,\n        drop_last=False)\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_set)), 3):\n        print(f\"Sample {index} of the training set: {train_set[index]}.\")\n    \n    # Optimizer\n    optimizer = torch.optim.AdamW(parameters, betas=[0.8,0.999], eps=1e-8, weight_decay=3e-7, lr=args.lr)\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation)\n    max_train_steps = args.epoch_num * num_update_steps_per_epoch\n    overrode_max_train_steps = True\n\n    # Create the learning rate scheduler.\n    # Note: the current accelerator.step() calls the .step() of the real scheduler for the `num_processes` times. This is because they assume \n    # the user initialize the scheduler with the entire training set. In the case of data parLayAlignl training, each process only\n    # sees a subset (1/num_processes) of the training set. So each time the process needs to update the lr multiple times so that the total \n    # number of updates in the end matches the num_training_steps here.\n    # Here we need to set the num_training_steps to either using the entire training set (when epochs is specified) or we need to multiply the \n    # num_training_steps by num_processes so that the total number of updates matches the num_training_steps.\n    num_training_steps_for_scheduler = max_train_steps if overrode_max_train_steps else max_train_steps * accelerator.num_processes\n    \"\"\"\n    get_scheduler Agrs\n    name:\n        LINEAR = \"linear\"\n        COSINE = \"cosine\"\n        COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n        POLYNOMIAL = \"polynomial\"\n        CONSTANT = \"constant\"\n        CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n        INVERSE_SQRT = \"inverse_sqrt\"\n        REDUCE_ON_PLATEAU = \"reduce_lr_on_plateau\"\n    \"\"\"\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_name,\n        optimizer=optimizer,\n        num_training_steps=num_training_steps_for_scheduler,\n        num_warmup_steps=int(num_training_steps_for_scheduler * args.warm_rate),\n    )\n\n\n    # Prepare everything with `accelerator`.\n    model, optimizer, train_dataloader, dev_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, dev_dataloader, lr_scheduler\n    )\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation)\n    if overrode_max_train_steps:\n        max_train_steps = epoch_num * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    epoch_num = math.ceil(max_train_steps / num_update_steps_per_epoch)\n    \n    # Train!\n    total_batch_size = train_micro_batch_size_per_gpu * accelerator.num_processes * gradient_accumulation\n\n    print(\"***** Running training *****\")\n    print(f\"  Num examples transet = {len(train_set)}\")\n    print(f\"  Num examples dataloader = {len(train_dataloader)}\")\n    print(f\"  Num Epochs = {epoch_num}\")\n    print(f\"  Instantaneous batch size per device = {train_micro_batch_size_per_gpu}\")\n    print(f\"  Total train batch size (w. parLayAlignl, distributed & accumulation) = {total_batch_size}\")\n    print(f\"  Gradient Accumulation steps = {gradient_accumulation}\")\n    print(f\"  Total optimization steps = {max_train_steps}\")\n    print(f\"  parameters = {parameters}\")\n    print(f\"  optimizer = {optimizer}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    progress_bar.update(completed_steps)\n    \n    for epoch in range(epoch_num):\n        model.train()\n        total_loss = 0\n        for batch in train_dataloader:\n            with accelerator.accumulate(model):\n                sources = batch['prompt']\n                targets = batch['target']\n\n                add_bos_token = False\n                add_eos_token = True\n                labels, mask_label = llm_input_features(targets, tokenizer_llm,\n                                                        max_gen_len, add_bos_token, add_eos_token)\n\n                input_ids_prompt, mask_prompt = None, None\n                if augmentation:\n                    add_bos_token = False\n                    add_eos_token = False\n    \n                    llm_input_prompts = [i for i in sources]\n                    \n                    # if args.system_prompt is not None:\n                    #     user_prompts = [\n                    #         [{\n                    #             \"role\": \"system\", \"content\": args.system_prompt\n                    #         },\n                    #         {\n                    #             \"role\": \"user\", \"content\": user_prompt_function(sources[i])\n                    #         }]\n                    #         for i in range(len(sources))\n                    #     ]\n\n                    #     llm_input_prompts = [tokenizer_llm.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True) for prompt in user_prompts]\n                        \n                    input_ids_prompt, mask_prompt = llm_input_features(llm_input_prompts, tokenizer_llm,\n                                                                        max_gen_len, add_bos_token,\n                                                                        add_eos_token)\n                output_loss = model(sources,\n                            input_ids_prompt=input_ids_prompt, mask_prompt=mask_prompt,\n                            labels=labels, mask_label=mask_label)\n                loss = output_loss\n                total_loss += output_loss.detach().float()\n                # We keep track of the loss at each logged step\n                accelerator.backward(loss)\n                # clip gradient norm. don't do this with deepspeed\n                optimizer.step()\n                optimizer.zero_grad()\n                lr_scheduler.step()\n\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n                \n                if args.logging_steps and completed_steps % args.logging_steps == 0:\n                    avg_loss = accelerator.gather(total_loss).mean().item() / gradient_accumulation / args.logging_steps\n                    total_loss = 0                   \n                    print(f\"  Step: {completed_steps}, LR: {lr_scheduler.get_last_lr()[0]}, Loss: {avg_loss}\")\n                  \n        epoch_model_path = f'./outputs/{save_name}/epoch_{epoch}_{stage_name}/'\n        save_with_accelerate(accelerator, model, epoch_model_path)\n        print('save epoch model')\n    accelerator.wait_for_everyone()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.205033Z","iopub.execute_input":"2025-04-27T10:53:53.205280Z","iopub.status.idle":"2025-04-27T10:53:53.225074Z","shell.execute_reply.started":"2025-04-27T10:53:53.205257Z","shell.execute_reply":"2025-04-27T10:53:53.224393Z"}},"outputs":[{"name":"stdout","text":"Writing trainer.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile eval.py\nimport torch.fx\nfrom transformers import AutoTokenizer\nimport torch\nimport argparse\nimport ast\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nimport json\nimport os\nimport sys\nimport deepspeed\nfrom transformers import AutoModelForCausalLM, AutoModel, AutoConfig\nimport math\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom torch import nn\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\nfrom huggingface_hub import login, upload_file\n\nfrom utils import get_train_ds_config, set_seed, evaluate_classification\nfrom model import MPTModel\nfrom read_data import ExperimentDataset, llm_input_features, read_dataset\n\ndef construct_prompt(sample):\n    return f\"### Instruction:\\nNews Sentence: {sample}\\nClassify the given news sentence into one of the following categories.\\nBusiness, Entertainment, Political, Sports, Science.\\n\\n### Response:\"\n\n\ndef main():\n    llm_path = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n    mt_path = \"google/mt5-large\"\n    max_seq_len = 512\n    max_gen_len = 512\n    eval_batch_size = 4\n    augmentation = False\n    save_name = \"no_aug\"\n    task = \"news\"\n        \n    \n    result_path_base = f'./results/{save_name}/{task}/'\n\n    token = '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'\n    login(token=token)\n\n    ds = load_dataset(\"Themira/en_si_news_classification_with_label_name\")\n\n    test_en = ds['test_en']\n    test_si = ds['test_si']\n    test_sets = {}\n    \n    test_set = []\n    for i in range(len(test_en)):\n        example = test_en[i]\n        sample = {}\n        sample['prompt'] = construct_prompt(example['sentence'])\n        sample['target'] = example['label']\n        sample['source_language'] = 'en'\n        test_set.append(sample)\n    test_sets['English'] = test_set\n    test_set = []\n    for i in range(len(test_si)):\n        example = test_si[i]\n        sample = {}\n        sample['prompt'] = construct_prompt(example['sentence'])\n        sample['target'] = example['label']\n        sample['source_language'] = 'si'\n        test_set.append(sample)\n    test_sets['Sinhala'] = test_set\n\n    os.makedirs(result_path_base, exist_ok=True)\n    tokenizer_llm = AutoTokenizer.from_pretrained(llm_path, use_fast=True)\n    tokenizer_llm.pad_token = tokenizer_llm.eos_token\n    tokenizer_llm.padding_side = \"left\"\n    # tokenizer_llm.pad_token = \"[PAD]\"\n    print(json.dumps({\n        'llm_path': llm_path,\n        'mt_path': mt_path,\n        'max_seq_len': max_seq_len,\n        'max_gen_len': max_gen_len,\n        'save_name': save_name,\n        'result_path_base': result_path_base\n    }, indent=2))\n    print(\"cuda available: \" , torch.cuda.is_available())\n    train_micro_batch_size_per_gpu = 4\n    train_batch_size = 4\n    gpu_num = torch.cuda.device_count()\n    gradient_accumulation = 1\n    # assert train_micro_batch_size_per_gpu * gpu_num * gradient_accumulation == train_batch_size\n    ds_config = get_train_ds_config(train_batch_size=train_batch_size,\n                                    train_micro_batch_size_per_gpu=train_micro_batch_size_per_gpu,\n                                    gradient_accumulation_steps=gradient_accumulation,\n                                    )\n\n    model_config = {\n        'mt_path': mt_path,\n        'llm_path': llm_path,\n        'max_gen_len': max_gen_len,\n        'llm_bos_token_id': tokenizer_llm.bos_token_id,\n        'llm_pad_token_id': tokenizer_llm.pad_token_id,\n        'augmentation' :  augmentation,\n        'max_seq_len': max_seq_len\n    }\n    init_checkpoint = \"./outputs/news/epoch_2_no_aug/pytorch_model.bin\"\n    model = MPTModel(model_config)\n    if init_checkpoint is not None:\n        init_checkpoint = init_checkpoint\n        checkpoint = torch.load(init_checkpoint, map_location='cpu')\n        #model_dict = checkpoint['model_state_dict']\n        model.load_state_dict(checkpoint, True)\n        # model.merge_loara()\n        print('mapping init from:', init_checkpoint)\n    model.to('cuda')\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    # model, optimizer, _, __ = deepspeed.initialize(\n    #     config=ds_config,\n    #     model=model,\n    #     model_parameters=parameters,\n    #     training_data=None)\n    scores_map = {}\n    avg = 0\n    for test_lang in test_sets:\n        test_set = test_sets[test_lang]\n        test_sampler = SequentialSampler(test_set)\n        test_set = ExperimentDataset(test_set)\n        test_set = torch.utils.data.DataLoader(\n            dataset=test_set,\n            batch_size=eval_batch_size,\n            sampler=test_sampler,\n            shuffle=False,\n            num_workers=1,\n            drop_last=False)\n        acc, results_list = evaluate_classification(model, test_set, tokenizer_llm, max_gen_len, augmentation)\n        \n        print('test_lang:', test_lang, 'acc:', acc)\n        scores_map[test_lang] = acc\n        result_path = f'{result_path_base}/{test_lang}.json'\n        with open(result_path, 'w', encoding='utf-8') as f:\n            json.dump(results_list, f, ensure_ascii=False, indent=2)\n        avg += acc\n    print(scores_map)\n    print('Average accuracy :', round(avg / len(test_sets), 1))\n    score_path = f'{result_path_base}/scores.tsv'\n    with open(score_path, 'w', encoding='utf-8') as f:\n        for lang in scores_map:\n            score = scores_map[lang]\n            f.write(f'{lang}\\t{score}\\n')\n\n\nif __name__ == \"__main__\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    set_seed(0)\n\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.225873Z","iopub.execute_input":"2025-04-27T10:53:53.226090Z","iopub.status.idle":"2025-04-27T10:53:53.243431Z","shell.execute_reply.started":"2025-04-27T10:53:53.226071Z","shell.execute_reply":"2025-04-27T10:53:53.242815Z"}},"outputs":[{"name":"stdout","text":"Writing eval.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile trainAndEval.sh\n#!/bin/bash\n\n# Use GPUs 0 and 1\nexport CUDA_VISIBLE_DEVICES=0\n\nrandom_port(){\n    # Random port\n    MASTER_PORT=$((30000 + RANDOM % (99999 - 30000 + 1)))\n    echo \"MASTER_PORT=$MASTER_PORT\"\n}\n\nexport_world_info() {\n    # Set world info for deepspeed\n    if [ -z \"$CUDA_VISIBLE_DEVICES\" ]; then\n        echo \"CUDA_VISIBLE_DEVICES is not set\"\n        NUM_GPUS=$(nvidia-smi -L | wc -l)\n        CUDA_VISIBLE_DEVICES=$(seq -s ',' 0 $((NUM_GPUS - 1)))\n        echo \"Use all GPUs\"\n        export \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\n        echo \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\n    else\n        NUM_GPUS=$(echo $CUDA_VISIBLE_DEVICES | tr ',' '\\n' | wc -l)\n        echo \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"\n    fi\n}\n\nrandom_port\nexport_world_info\n\n# Train\naccelerate launch \\\n    --main_process_port $MASTER_PORT \\\n    --num_machines 1 \\\n    --num_processes $NUM_GPUS \\\n    --use_deepspeed \\\n    --deepspeed_config_file dp.json \\\n    trainer.py \\\n\n# Evaluate\npython eval.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:53:53.244465Z","iopub.execute_input":"2025-04-27T10:53:53.244782Z","iopub.status.idle":"2025-04-27T10:53:53.262904Z","shell.execute_reply.started":"2025-04-27T10:53:53.244750Z","shell.execute_reply":"2025-04-27T10:53:53.262129Z"}},"outputs":[{"name":"stdout","text":"Writing trainAndEval.sh\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!bash trainAndEval.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:56:44.294940Z","iopub.execute_input":"2025-04-27T10:56:44.295602Z","iopub.status.idle":"2025-04-27T11:59:23.852408Z","shell.execute_reply.started":"2025-04-27T10:56:44.295575Z","shell.execute_reply":"2025-04-27T11:59:23.851704Z"}},"outputs":[{"name":"stdout","text":"MASTER_PORT=37288\nCUDA_VISIBLE_DEVICES=0\n[2025-04-27 10:56:50,572] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n2025-04-27 10:56:53.016774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745751413.040441     479 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745751413.047771     479 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-27 10:57:01.365677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745751421.390555     525 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745751421.399791     525 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2025-04-27 10:57:04,224] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2025-04-27 10:57:05,610] [INFO] [comm.py:669:init_distributed] cdb=None\n[2025-04-27 10:57:05,610] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n[rank0]:[W427 10:57:05.276861938 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n{\n  \"llm_path\": \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n  \"mt_path\": \"google/mt5-large\",\n  \"lr\": 3e-05,\n  \"epoch_num\": 3,\n  \"gradient_accumulation\": 1,\n  \"train_set:\": 17787,\n  \"dev_set:\": 1000,\n  \"max_seq_len\": 200,\n  \"max_gen_len\": 200,\n  \"train_batch_size\": 8,\n  \"result_path\": \"./results/news/no_aug/\",\n  \"output_model_path\": \"./outputs/news/no_aug/\"\n}\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nmapping layer size: 3.28\nMPTModel(\n  (encoder_mt): MultilingualEmbeddingModel(\n    (embedding_model): MT5Stack(\n      (embed_tokens): Embedding(250112, 1024)\n      (block): ModuleList(\n        (0): MT5Block(\n          (layer): ModuleList(\n            (0): MT5LayerSelfAttention(\n              (SelfAttention): MT5Attention(\n                (q): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (k): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (v): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (o): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): MT5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): MT5LayerFF(\n              (DenseReluDense): MT5DenseGatedActDense(\n                (wi_0): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=2816, bias=False)\n                )\n                (wi_1): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=2816, bias=False)\n                )\n                (wo): BiasTunedLinear(\n                  (linear): Linear(in_features=2816, out_features=1024, bias=False)\n                )\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): MT5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-19): 19 x MT5Block(\n          (layer): ModuleList(\n            (0): MT5LayerSelfAttention(\n              (SelfAttention): MT5Attention(\n                (q): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (k): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (v): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (o): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n              )\n              (layer_norm): MT5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): MT5LayerFF(\n              (DenseReluDense): MT5DenseGatedActDense(\n                (wi_0): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=2816, bias=False)\n                )\n                (wi_1): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=2816, bias=False)\n                )\n                (wo): BiasTunedLinear(\n                  (linear): Linear(in_features=2816, out_features=1024, bias=False)\n                )\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): MT5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (20-23): 4 x MT5Block(\n          (layer): ModuleList(\n            (0): MT5LayerSelfAttention(\n              (SelfAttention): GatedMT5Attention(\n                (q): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (k): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (v): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n                (o): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=1024, bias=False)\n                )\n              )\n              (layer_norm): MT5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): MT5LayerFF(\n              (DenseReluDense): MT5DenseGatedActDense(\n                (wi_0): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=2816, bias=False)\n                )\n                (wi_1): BiasTunedLinear(\n                  (linear): Linear(in_features=1024, out_features=2816, bias=False)\n                )\n                (wo): BiasTunedLinear(\n                  (linear): Linear(in_features=2816, out_features=1024, bias=False)\n                )\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): MT5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): MT5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (model_llm): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(49152, 576, padding_idx=2)\n      (layers): ModuleList(\n        (0-29): 30 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear(in_features=576, out_features=576, bias=False)\n            (k_proj): Linear(in_features=576, out_features=192, bias=False)\n            (v_proj): Linear(in_features=576, out_features=192, bias=False)\n            (o_proj): Linear(in_features=576, out_features=576, bias=False)\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n            (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n            (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((576,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n  )\n  (llm_embedding_layer): Embedding(49152, 576, padding_idx=2)\n  (mapping): Mapping(\n    (mlp): MLP(\n      (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n      (linear2): Linear(in_features=2048, out_features=576, bias=True)\n      (relu): ReLU()\n    )\n  )\n)\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.0.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.1.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.2.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.3.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.4.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.5.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.6.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.7.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.8.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.9.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.10.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.11.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.12.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.13.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.14.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.15.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.16.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.17.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.18.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.19.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.gate, requires_grad=True, shape=torch.Size([16])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.prompts, requires_grad=True, shape=torch.Size([10, 1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.20.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.gate, requires_grad=True, shape=torch.Size([16])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.prompts, requires_grad=True, shape=torch.Size([10, 1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.21.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.gate, requires_grad=True, shape=torch.Size([16])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.prompts, requires_grad=True, shape=torch.Size([10, 1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.22.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.gate, requires_grad=True, shape=torch.Size([16])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.prompts, requires_grad=True, shape=torch.Size([10, 1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.q.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.q.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.k.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.k.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.v.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.v.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.o.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.SelfAttention.o.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.0.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.DenseReluDense.wi_0.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.DenseReluDense.wi_0.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.DenseReluDense.wi_1.bias_delta, requires_grad=True, shape=torch.Size([2816])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.DenseReluDense.wi_1.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.DenseReluDense.wo.bias_delta, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.DenseReluDense.wo.scale_tuned, requires_grad=True, shape=torch.Size([1])\nParameter name: encoder_mt.embedding_model.block.23.layer.1.layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: encoder_mt.embedding_model.final_layer_norm.weight, requires_grad=True, shape=torch.Size([1024])\nParameter name: mapping.end_boundary, requires_grad=True, shape=torch.Size([1, 1, 576])\nParameter name: mapping.mlp.linear1.weight, requires_grad=True, shape=torch.Size([2048, 1024])\nParameter name: mapping.mlp.linear1.bias, requires_grad=True, shape=torch.Size([2048])\nParameter name: mapping.mlp.linear2.weight, requires_grad=True, shape=torch.Size([576, 2048])\nParameter name: mapping.mlp.linear2.bias, requires_grad=True, shape=torch.Size([576])\nSample 13781 of the training set: {'prompt': \"### Instruction:\\nNews Sentence: Dodgers Eagle Celebrates Independence By Flying Away During Pregame Ceremony The eagle hasn't landed.\\nClassify the given news sentence into one of the following categories.\\nBusiness, Entertainment, Political, Sports, Science.\\n\\n### Response:\", 'target': 'Sports'}.\nSample 1326 of the training set: {'prompt': '### Instruction:\\nNews Sentence: Bernie Sanders Floats 50-Percent Cut On Prescription Drugs If Elected Other 2020 hopefuls including Amy Klobuchar, Kirsten Gillibrand and Elizabeth Warren have proposed measures to clamp down on the pharmaceutical industry.\\nClassify the given news sentence into one of the following categories.\\nBusiness, Entertainment, Political, Sports, Science.\\n\\n### Response:', 'target': 'Political'}.\nSample 8484 of the training set: {'prompt': '### Instruction:\\nNews Sentence: Ryan Gosling Laughed During Oscars Mix-Up Because He\\'d Thought Someone Had Been Hurt \"I had this worst-case scenario playing out in my head.\"\\nClassify the given news sentence into one of the following categories.\\nBusiness, Entertainment, Political, Sports, Science.\\n\\n### Response:', 'target': 'Entertainment'}.\n[2025-04-27 10:57:31,107] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n[2025-04-27 10:57:31,107] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1\n[2025-04-27 10:57:32,349] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n[2025-04-27 10:57:32,351] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n[2025-04-27 10:57:32,351] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n[2025-04-27 10:57:32,382] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n[2025-04-27 10:57:32,382] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n[2025-04-27 10:57:32,382] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n[2025-04-27 10:57:32,382] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n[2025-04-27 10:57:32,382] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n[2025-04-27 10:57:32,382] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n[2025-04-27 10:57:32,382] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: False\n[2025-04-27 10:57:32,825] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n[2025-04-27 10:57:32,825] [INFO] [utils.py:782:see_memory_usage] MA 1.34 GB         Max_MA 1.35 GB         CA 1.4 GB         Max_CA 1 GB \n[2025-04-27 10:57:32,826] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.57 GB, percent = 17.8%\n[2025-04-27 10:57:33,227] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n[2025-04-27 10:57:33,228] [INFO] [utils.py:782:see_memory_usage] MA 1.34 GB         Max_MA 1.36 GB         CA 1.4 GB         Max_CA 1 GB \n[2025-04-27 10:57:33,228] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.57 GB, percent = 17.8%\n[2025-04-27 10:57:33,229] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized\n[2025-04-27 10:57:33,628] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n[2025-04-27 10:57:33,628] [INFO] [utils.py:782:see_memory_usage] MA 1.34 GB         Max_MA 1.34 GB         CA 1.4 GB         Max_CA 1 GB \n[2025-04-27 10:57:33,629] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.57 GB, percent = 17.8%\n[2025-04-27 10:57:33,636] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n[2025-04-27 10:57:33,636] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n[2025-04-27 10:57:33,636] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n[2025-04-27 10:57:33,636] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.8, 0.999]]\n[2025-04-27 10:57:33,638] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n    \"partition_activations\": false, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   amp_enabled .................. False\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   amp_params ................... False\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   autotuning_config ............ {\n    \"enabled\": false, \n    \"start_step\": null, \n    \"end_step\": null, \n    \"metric_path\": null, \n    \"arg_mappings\": null, \n    \"metric\": \"throughput\", \n    \"model_info\": null, \n    \"results_dir\": \"autotuning_results\", \n    \"exps_dir\": \"autotuning_exps\", \n    \"overwrite\": true, \n    \"fast\": true, \n    \"start_profile_step\": 3, \n    \"end_profile_step\": 5, \n    \"tuner_type\": \"gridsearch\", \n    \"tuner_early_stopping\": 5, \n    \"tuner_num_trials\": 50, \n    \"model_info_path\": null, \n    \"mp_size\": 1, \n    \"max_train_batch_size\": null, \n    \"min_train_batch_size\": 1, \n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n    \"min_train_micro_batch_size_per_gpu\": 1, \n    \"num_tuning_micro_batch_sizes\": 3\n}\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n[2025-04-27 10:57:33,639] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbc95064dd0>\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   communication_data_type ...... None\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   disable_allgather ............ False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   dump_state ................... False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n[2025-04-27 10:57:33,640] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"recompute_fwd_factor\": 0.0, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   global_rank .................. 0\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n[2025-04-27 10:57:33,641] [INFO] [config.py:1007:print]   nebula_config ................ {\n    \"enabled\": false, \n    \"persistent_storage_path\": null, \n    \"persistent_time_interval\": 100, \n    \"num_of_version_in_retention\": 2, \n    \"enable_nebula_load\": true, \n    \"load_path\": null\n}\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   optimizer_name ............... None\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   optimizer_params ............. None\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   pld_enabled .................. False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   pld_params ................... False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   scheduler_name ............... None\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   scheduler_params ............. None\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   sparse_attention ............. None\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   train_batch_size ............. 8\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  8\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   world_size ................... 1\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n[2025-04-27 10:57:33,642] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n[2025-04-27 10:57:33,643] [INFO] [config.py:1007:print]   zero_enabled ................. True\n[2025-04-27 10:57:33,643] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n[2025-04-27 10:57:33,643] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2\n[2025-04-27 10:57:33,643] [INFO] [config.py:993:print_user_config]   json = {\n    \"train_batch_size\": 8, \n    \"train_micro_batch_size_per_gpu\": 8, \n    \"steps_per_print\": inf, \n    \"zero_optimization\": {\n        \"stage\": 2, \n        \"offload_param\": {\n            \"device\": \"none\"\n        }, \n        \"offload_optimizer\": {\n            \"device\": \"none\"\n        }, \n        \"stage3_param_persistence_threshold\": 1.000000e+04, \n        \"stage3_max_live_parameters\": 3.000000e+07, \n        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n        \"memory_efficient_linear\": false\n    }, \n    \"bf16\": {\n        \"enabled\": true, \n        \"loss_scale_window\": 50, \n        \"min_loss_scale\": 1e-10\n    }, \n    \"gradient_clipping\": 1.0, \n    \"gradient_accumulation_steps\": 1, \n    \"prescale_gradients\": false, \n    \"wall_clock_breakdown\": false, \n    \"hybrid_engine\": {\n        \"enabled\": false, \n        \"max_out_tokens\": 512, \n        \"inference_tp_size\": 1, \n        \"release_inference_cache\": false, \n        \"pin_parameters\": true, \n        \"tp_gather_partition_size\": 8\n    }, \n    \"fp16\": {\n        \"enabled\": false\n    }, \n    \"zero_allow_untested_optimizer\": true\n}\n***** Running training *****\n  Num examples transet = 17787\n  Num examples dataloader = 2224\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parLayAlignl, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 6672\n  parameters = <filter object at 0x7fbc94d635e0>\n  optimizer = DeepSpeedOptimizerWrapper (\nParameter Group 0\n    amsgrad: False\n    betas: [0.8, 0.999]\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    initial_lr: 3e-05\n    lr: 0.0\n    maximize: False\n    weight_decay: 3e-07\n)\n  1%|▌                                       | 100/6672 [00:49<54:07,  2.02it/s]  Step: 100, LR: 9.00900900900901e-06, Loss: 10.523887939453125\n  3%|█▏                                      | 200/6672 [01:39<50:41,  2.13it/s]  Step: 200, LR: 1.801801801801802e-05, Loss: 1.7939894104003906\n  4%|█▊                                      | 300/6672 [02:28<48:33,  2.19it/s]  Step: 300, LR: 2.7027027027027027e-05, Loss: 0.9331018829345703\n  6%|██▍                                     | 400/6672 [03:19<52:04,  2.01it/s]  Step: 400, LR: 2.99917314528587e-05, Loss: 0.8029603576660156\n  7%|██▉                                     | 500/6672 [04:08<50:59,  2.02it/s]  Step: 500, LR: 2.9948654247476475e-05, Loss: 0.6747651672363282\n  9%|███▌                                    | 600/6672 [04:58<50:01,  2.02it/s]  Step: 600, LR: 2.9868868135796024e-05, Loss: 0.6231488037109375\n 10%|████▏                                   | 700/6672 [05:48<49:22,  2.02it/s]  Step: 700, LR: 2.9752569045882117e-05, Loss: 0.5782270812988282\n 12%|████▊                                   | 800/6672 [06:38<46:25,  2.11it/s]  Step: 800, LR: 2.960004256948926e-05, Loss: 0.4872099304199219\n 13%|█████▍                                  | 900/6672 [07:27<49:53,  1.93it/s]  Step: 900, LR: 2.9411663260743684e-05, Loss: 0.4437768173217773\n 15%|█████▊                                 | 1000/6672 [08:17<48:21,  1.95it/s]  Step: 1000, LR: 2.918789371636338e-05, Loss: 0.4197093963623047\n 16%|██████▍                                | 1100/6672 [09:07<50:57,  1.82it/s]  Step: 1100, LR: 2.8929283439674936e-05, Loss: 0.40538318634033205\n 18%|███████                                | 1200/6672 [09:56<45:18,  2.01it/s]  Step: 1200, LR: 2.8636467491216757e-05, Loss: 0.3605218505859375\n 19%|███████▌                               | 1300/6672 [10:47<45:05,  1.99it/s]  Step: 1300, LR: 2.8310164929242283e-05, Loss: 0.31909278869628904\n 21%|████████▏                              | 1400/6672 [11:38<44:57,  1.95it/s]  Step: 1400, LR: 2.7951177043952897e-05, Loss: 0.37571403503417966\n 22%|████████▊                              | 1500/6672 [12:27<41:13,  2.09it/s]  Step: 1500, LR: 2.7560385389796597e-05, Loss: 0.3370474243164063\n 24%|█████████▎                             | 1600/6672 [13:16<43:17,  1.95it/s]  Step: 1600, LR: 2.7138749620664428e-05, Loss: 0.30960674285888673\n 25%|█████████▉                             | 1700/6672 [14:05<40:59,  2.02it/s]  Step: 1700, LR: 2.668730513330079e-05, Loss: 0.29721940994262697\n 27%|██████████▌                            | 1800/6672 [14:53<42:05,  1.93it/s]  Step: 1800, LR: 2.620716052471456e-05, Loss: 0.2940241241455078\n 28%|███████████                            | 1900/6672 [15:44<37:40,  2.11it/s]  Step: 1900, LR: 2.5699494869834793e-05, Loss: 0.2523995971679687\n 30%|███████████▋                           | 2000/6672 [16:33<43:23,  1.79it/s]  Step: 2000, LR: 2.516555482609614e-05, Loss: 0.2637181854248047\n 31%|████████████▎                          | 2100/6672 [17:23<41:14,  1.85it/s]  Step: 2100, LR: 2.4606651572064252e-05, Loss: 0.2682087135314941\n 33%|████████████▊                          | 2200/6672 [18:13<40:21,  1.85it/s]  Step: 2200, LR: 2.4024157587618738e-05, Loss: 0.2907840728759766\n 33%|█████████████                          | 2224/6672 [18:25<31:54,  2.32it/s]save epoch model\n 34%|█████████████▍                         | 2300/6672 [19:06<35:48,  2.04it/s]  Step: 2300, LR: 2.3419503283600627e-05, Loss: 0.20528575897216797\n 36%|██████████████                         | 2400/6672 [19:55<33:23,  2.13it/s]  Step: 2400, LR: 2.2794173489200666e-05, Loss: 0.2485882568359375\n 37%|██████████████▌                        | 2500/6672 [20:45<33:32,  2.07it/s]  Step: 2500, LR: 2.2149703805714335e-05, Loss: 0.27380859375\n 39%|███████████████▏                       | 2600/6672 [21:35<34:45,  1.95it/s]  Step: 2600, LR: 2.148767683561751e-05, Loss: 0.2599286842346191\n 40%|███████████████▊                       | 2700/6672 [22:26<35:44,  1.85it/s]  Step: 2700, LR: 2.0809718296222933e-05, Loss: 0.23891807556152345\n 42%|████████████████▎                      | 2800/6672 [23:15<31:56,  2.02it/s]  Step: 2800, LR: 2.011749302746098e-05, Loss: 0.25070144653320314\n 43%|████████████████▉                      | 2900/6672 [24:04<31:34,  1.99it/s]  Step: 2900, LR: 1.94127009035884e-05, Loss: 0.2391347122192383\n 45%|█████████████████▌                     | 3000/6672 [24:55<29:17,  2.09it/s]  Step: 3000, LR: 1.869707265886439e-05, Loss: 0.2380111312866211\n 46%|██████████████████                     | 3100/6672 [25:46<32:26,  1.84it/s]  Step: 3100, LR: 1.7972365637444817e-05, Loss: 0.2334243392944336\n 48%|██████████████████▋                    | 3200/6672 [26:35<29:14,  1.98it/s]  Step: 3200, LR: 1.724035947793143e-05, Loss: 0.20116670608520507\n 49%|███████████████████▎                   | 3300/6672 [27:24<29:21,  1.91it/s]  Step: 3300, LR: 1.650285174317332e-05, Loss: 0.22080718994140625\n 51%|███████████████████▊                   | 3400/6672 [28:13<24:07,  2.26it/s]  Step: 3400, LR: 1.5761653506052462e-05, Loss: 0.22649486541748046\n 52%|████████████████████▍                  | 3500/6672 [29:03<26:52,  1.97it/s]  Step: 3500, LR: 1.5018584902093057e-05, Loss: 0.24125890731811522\n 54%|█████████████████████                  | 3600/6672 [29:51<26:55,  1.90it/s]  Step: 3600, LR: 1.4275470659816101e-05, Loss: 0.23546770095825195\n 55%|█████████████████████▋                 | 3700/6672 [30:42<26:09,  1.89it/s]  Step: 3700, LR: 1.3534135619815072e-05, Loss: 0.23749277114868164\n 57%|██████████████████████▏                | 3800/6672 [31:31<24:16,  1.97it/s]  Step: 3800, LR: 1.2796400253556394e-05, Loss: 0.23334817886352538\n 58%|██████████████████████▊                | 3900/6672 [32:20<22:33,  2.05it/s]  Step: 3900, LR: 1.2064076192909031e-05, Loss: 0.2485032844543457\n 60%|███████████████████████▍               | 4000/6672 [33:08<21:22,  2.08it/s]  Step: 4000, LR: 1.133896178138118e-05, Loss: 0.209272518157959\n 61%|███████████████████████▉               | 4100/6672 [33:59<23:14,  1.84it/s]  Step: 4100, LR: 1.0622837657988747e-05, Loss: 0.21528472900390624\n 63%|████████████████████████▌              | 4200/6672 [34:48<21:00,  1.96it/s]  Step: 4200, LR: 9.917462384600128e-06, Loss: 0.2590955924987793\n 64%|█████████████████████████▏             | 4300/6672 [35:39<19:19,  2.05it/s]  Step: 4300, LR: 9.224568127495132e-06, Loss: 0.21981447219848632\n 66%|█████████████████████████▋             | 4400/6672 [36:28<18:40,  2.03it/s]  Step: 4400, LR: 8.545856403742615e-06, Loss: 0.23070405960083007\n 67%|██████████████████████████             | 4448/6672 [36:51<16:24,  2.26it/s]save epoch model\n 67%|██████████████████████████▎            | 4500/6672 [37:21<17:09,  2.11it/s]  Step: 4500, LR: 7.882993902842403e-06, Loss: 0.0915478515625\n 69%|██████████████████████████▉            | 4600/6672 [38:09<16:57,  2.04it/s]  Step: 4600, LR: 7.2376083938920614e-06, Loss: 0.23140472412109375\n 70%|███████████████████████████▍           | 4700/6672 [38:59<16:20,  2.01it/s]  Step: 4700, LR: 6.611284728329187e-06, Loss: 0.2129378128051758\n 72%|████████████████████████████           | 4800/6672 [39:48<16:07,  1.94it/s]  Step: 4800, LR: 6.00556094806517e-06, Loss: 0.1792873764038086\n 73%|████████████████████████████▋          | 4900/6672 [40:38<13:53,  2.13it/s]  Step: 4900, LR: 5.421924508567519e-06, Loss: 0.23075178146362305\n 75%|█████████████████████████████▏         | 5000/6672 [41:27<14:46,  1.89it/s]  Step: 5000, LR: 4.861808626165618e-06, Loss: 0.1724630355834961\n 76%|█████████████████████████████▊         | 5100/6672 [42:16<13:05,  2.00it/s]  Step: 5100, LR: 4.32658875854969e-06, Loss: 0.21699670791625977\n 78%|██████████████████████████████▍        | 5200/6672 [43:06<12:15,  2.00it/s]  Step: 5200, LR: 3.81757922710573e-06, Loss: 0.21061315536499023\n 79%|██████████████████████████████▉        | 5300/6672 [43:55<10:59,  2.08it/s]  Step: 5300, LR: 3.3360299893807745e-06, Loss: 0.18442804336547852\n 81%|███████████████████████████████▌       | 5400/6672 [44:44<10:17,  2.06it/s]  Step: 5400, LR: 2.8831235696042964e-06, Loss: 0.21004249572753905\n 82%|████████████████████████████████▏      | 5500/6672 [45:33<08:42,  2.24it/s]  Step: 5500, LR: 2.4599721548033122e-06, Loss: 0.21776334762573243\n 84%|████████████████████████████████▋      | 5600/6672 [46:22<08:25,  2.12it/s]  Step: 5600, LR: 2.067614863642198e-06, Loss: 0.19680133819580078\n 85%|█████████████████████████████████▎     | 5700/6672 [47:12<07:49,  2.07it/s]  Step: 5700, LR: 1.707015194694025e-06, Loss: 0.15305134773254395\n 87%|█████████████████████████████████▉     | 5800/6672 [48:02<07:42,  1.88it/s]  Step: 5800, LR: 1.3790586604095695e-06, Loss: 0.20059555053710937\n 88%|██████████████████████████████████▍    | 5900/6672 [48:51<06:24,  2.01it/s]  Step: 5900, LR: 1.0845506125942662e-06, Loss: 0.20589460372924806\n 90%|███████████████████████████████████    | 6000/6672 [49:42<05:32,  2.02it/s]  Step: 6000, LR: 8.242142647329071e-07, Loss: 0.19265369415283204\n 91%|███████████████████████████████████▋   | 6100/6672 [50:32<05:03,  1.88it/s]  Step: 6100, LR: 5.986889160186809e-07, Loss: 0.1743752861022949\n 93%|████████████████████████████████████▏  | 6200/6672 [51:21<03:28,  2.26it/s]  Step: 6200, LR: 4.085283814476937e-07, Loss: 0.18438575744628907\n 94%|████████████████████████████████████▊  | 6300/6672 [52:11<02:45,  2.25it/s]  Step: 6300, LR: 2.5419963183414717e-07, Loss: 0.17433584213256836\n 96%|█████████████████████████████████████▍ | 6400/6672 [53:01<02:12,  2.05it/s]  Step: 6400, LR: 1.3608164708585725e-07, Loss: 0.18487634658813476\n 97%|█████████████████████████████████████▉ | 6500/6672 [53:51<01:32,  1.87it/s]  Step: 6500, LR: 5.446448555606998e-08, Loss: 0.21960878372192383\n 99%|██████████████████████████████████████▌| 6600/6672 [54:39<00:31,  2.31it/s]  Step: 6600, LR: 9.54857175694046e-09, Loss: 0.21532415390014648\n100%|███████████████████████████████████████| 6672/6672 [55:16<00:00,  2.37it/s]save epoch model\n100%|███████████████████████████████████████| 6672/6672 [55:20<00:00,  2.01it/s]\n[rank0]:[W427 11:52:54.285073460 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[2025-04-27 11:53:03,347] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n2025-04-27 11:53:07.297881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745754787.322870     634 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745754787.330700     634 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n{\n  \"llm_path\": \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n  \"mt_path\": \"google/mt5-large\",\n  \"max_seq_len\": 512,\n  \"max_gen_len\": 512,\n  \"save_name\": \"no_aug\",\n  \"result_path_base\": \"./results/no_aug/news/\"\n}\ncuda available:  True\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nmapping layer size: 3.28\n/kaggle/working/eval.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(init_checkpoint, map_location='cpu')\nmapping init from: ./outputs/news/epoch_2_no_aug/pytorch_model.bin\n100%|████████████████████████████| 1566/1566 [03:53<00:00,  6.71it/s, Acc:88.26]\ntest_lang: English acc: 88.26\n100%|██████████████████████████████| 112/112 [01:54<00:00,  1.02s/it, Acc:77.98]\ntest_lang: Sinhala acc: 77.98\n{'English': 88.26, 'Sinhala': 77.98}\nAverage accuracy : 83.1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}